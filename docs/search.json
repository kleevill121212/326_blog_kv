[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/05-pvalues/index.html",
    "href": "posts/05-pvalues/index.html",
    "title": "Mini Project 5",
    "section": "",
    "text": "Advantages and Drawbacks of Using p-values\nMathematical Statistics Mini Project 5\nKobe Villeneuve\nQuestion 1\nI believe the author is referencing the point of focusing solely on statistical significance as a measure of validity for research. As mentioned in the opening statements of the paper, two of the ‘Don’ts’ can be summarized as: do not let the p-value sway you in either direction when determining whether something can be derived from the relationships discovered in research. When researchers focus too much on statistically significant relationships, discovery can be lost in that pursuit. I can see it in my research when parsing through volumes of data, searching for ‘significant’ relationships. While these correlations with strong significance bear some value to the research being done, it does not mean I should be discarding the work done that produced higher p-values. I see statistical thinking as using all of the tools in your statistical toolkit, with statistical significance being one of them. Other tools include intuition about the relationships at play, qualitative aspects of the data that may not be accurately explained by the numbers that represent them, and, of course, the complexity of the data. Statistical thinking involves thinking critically about your data and how it is produced. An example of this would be examining trade centrality and global influence. When I examine the relationship between these two sets, there is a clear and significant relationship. However, when I think critically about my ‘statistics’, I can see there is a strong internal impact of economic influence on global influence. So it is clear that, in a sense, these two categories are measuring something similar, so it dampens the significance of the relationship. Conversely, a relationship between trade centrality and GDP per capita was found to have no significance at all. However, this is largely since GDP per capita is an upward sloping trend over time for developed nations, and centrality varies very marginally. The insignificance of the regression analysis does not mean I should discard the analysis entirely, but rather think more about how I can measure the relationship differently. Case in point, I think what the authors are communicating here is that significance is a tool, to be used inharmony with other tools, to allow a statistician (or math honors student) to conduct meaningful research that produces insightful conclusions.\nQuestion 2\nI think that by the first portion of their statement, the author means the p-value communicates what needs to be received. If a reader understands how to interpret a p-value properly (if they took Stat 326, for example), then they would be able to make the proper association between the p-value and the data. To set a line in the sand and say “this side is significant” and “the other side is not” takes away from the richness of the findings. It creates a mental barrier for consumers of the literature or research to discard the insignificant findings, for the simple reason mentioned earlier in the paper of how entangled the statistical and plain English definitions of significance have become. When creating a boundary or a dichotomy, it removes the element of critical thinking when digesting information. It seems like you have already been told that the data is ‘good’ or ‘bad’ because of the connotations associated with words like significant or insignificant. The label creates the dichotomization because it creates the two groups. If you are under, you are safe, if you are over, then you are not. I think the author means that by creating these two groups, there is too much emphasis placed on the categorization and not enough on the rest of the analysis. This is something I encounter when doing my own research, because I have been taught what is ‘good’ and what is ‘bad’, or at least that is how I interpreted the dichotomy throughout my education. It stands to reason that one should deploy the toolkit I mentioned earlier to enhance their comprehension of data.\nQuestion 3\nI agree mostly with this claim presented by the authors. When deciding what to include in published research, findings need to be evaluated on relevance, depth of research, and quality of data. I think that if the data is collected in an unbiased manner, with a large sample size, and all qualitative factors point to the results being relevant, then the findings should be included. You can cook up any number of relationships that have a low p-value, which have nothing to dowith each other, if you examine the qualitative details of the data. Say, for example, we look at coffee consumption and broken bones, which somehow have a strong relationship with a low p-value. If you examine that qualitatively, or think critically about what you are looking at, I am sure you could conclude that you might want to include something more relevant in your publication. However, I do feel that by examining what kind of research you are doing, the error types in the context of the problem present a fair argument for caring about a p-value to a certain degree. If you want to err on the side of caution and bias more towards an error type that is less harmful, like the arsenic concentration example we have looked at, then considering the p-value may help you with that effort. So while I largely believe what the authors are telling us, I think that the p-value is a tool that can be useful at times to make certain decisions.\nQuestion 4\nI agree wholeheartedly with this statement. A one-size-fits-all approach rarely works for anything, statistics aside. When dealing with data, it is often messy, with a great deal of uncertainty at play. I believe that expecting the certainty of one approach that is best is not realistic. The ATOM method they describe makes sense, because it is more a way of thinking that strays away from the binary decision making of a silver bullet solution. I think that it will always be context-specific, since statistical analysis spreads across every single field in the world. You would not be able to follow the same set of rules when dealing with medical research as you would with consumer preferences for a certain type of fabric for shirts. The context will always alter the approach you have to take when making decisions about your results and interpretation of those results, thus, it is not a reasonable expectation to have one singular method or rule to dictate the usefulness of findings.\nQuestion 5\nThe authors reference ‘statistical thoughtfulness’ as a way of creating an approach to statistical analysis that requires deeper thought in all aspects of conducting research. I think this thoughtfulness means to consider why you are analyzing something, how you will do it, how youwill collect the data, and how the results will impact the current thought on your topic. When you consider what has been done before you in addition to where the community stands on a topic, it frees you up to make a calculated approach to your methods. I’ll draw on a personal example to highlight what it means to be statistically thoughtful. When I was thinking about my research question, I needed to first determine what was out there on my topic. There was plenty of writing done on modelling trade networks, as well as more complex mathematical analysis to predict outcomes of certain systems. Once I knew this, I was able to sharpen and define my goal clearly as an exploratory study on the relationships that clique centrality has with other economic indicators. This freed me up to include all relevant economic indicators, because I knew that a study of this manner did not exist in the popular literature, so although only a few variables held statistical significance with each other, I was able to produce meaningful conclusions. My general interpretation of this thoughtfulness is being deliberate in your decision-making to ensure you are contributing to the broader community.\nQuestion 6\nI agree with the authors that there is a problem with the way statistical results are presented. I think that the problem which the authors are referencing is directly related to the interpretation of those who read it. By stating conclusions using significant or confident language, it overstates the certainty of the claim to the reader. If every person on Earth took statistics courses in their educational journey, it might be okay to leave things as they are. The challenge arises when folks with non-statistical backgrounds interpret the research improperly. I think the term ‘compatibility interval’ is also a bit misleading. I believe that adding an interpretation with probabilities, whether the null is true, could be easier for people to interpret, thus making things more digestible to a broader audience. Compatibility does not introduce much more than the other terms in my opinion, but stating things such as “There is a 70% chance that the diet reduces weight” provides the necessary color.\nQuestion 7\n\nFound in Section 2, paragraph 4.\nI find this quote particularly interesting because it feels paradoxical. It made me think about how you would measure the difference between significant and not significant. Maybe a null hypothesis that there is no difference, and an alternative that there is. If you conduct this test and it tells you that the difference is not statistically significant, do you even trust that interpretation? It challenges statistical significance by using statistical significance. I really like how ‘meta’ this quote felt. It also made me think about how you go about proving the validity of analysis. I think that thoughtfulness, openness, and modesty are all great practices in statistical reporting (and life in general). Pre-specified power, novelty, and other measures that you can evaluate also help in determining how you analyze your data - it just seems like quite a lot of legwork to make these adjustments, although I agree they should be made!"
  },
  {
    "objectID": "posts/01-simulation/index.html",
    "href": "posts/01-simulation/index.html",
    "title": "Mini Project 1",
    "section": "",
    "text": "Sampling Distribution of the Sample Minimum and Maximum\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## Min for Normal\n\nn &lt;- 5 # sample size\nmu &lt;- 10 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_min &lt;- function(mu, sigma, n) {single_sample &lt;- rnorm(n, mu, sigma)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 5.441205\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  5.30\n 2  9.18\n 3  7.37\n 4 10.4 \n 5  8.39\n 6  9.45\n 7  9.06\n 8  9.95\n 9  8.02\n10  6.26\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          7.70          1.72         1.31\n\n## Max for Normal\n\nn &lt;- 5 # sample size\nmu &lt;- 10 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_max &lt;- function(mu, sigma, n) {single_sample &lt;- rnorm(n, mu, sigma)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 10.66192\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  11.9\n 2  12.3\n 3  10.3\n 4  12.7\n 5  14.5\n 6  11.8\n 7  10.3\n 8  13.9\n 9  13.6\n10  12.4\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(min_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          12.3          1.73         1.31\n\nlibrary(tidyverse)\n\n## Min for Uniform\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7 \ntheta2 &lt;- 13 \nmu = (theta1+theta2)/2\nsigma = sqrt((theta2-theta1)^2 / 12)\ngenerate_samp_min &lt;- function(mu, sigma, n) {single_sample &lt;- runif(n,theta1, theta2)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 7.669392\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  7.86\n 2  8.57\n 3  7.70\n 4  8.19\n 5  7.59\n 6  7.63\n 7  8.05\n 8  9.77\n 9  8.09\n10  7.03\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          8.00         0.729        0.854\n\n## Max for the Uniform\nlibrary(tidyverse)\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7 \ntheta2 &lt;- 13 \nmu = (theta1+theta2)/2\nsigma = sqrt((theta2-theta1)^2 / 12)\ngenerate_samp_max &lt;- function(mu, sigma, n) {single_sample &lt;- runif(n,theta1, theta2)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 12.65147\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmaxs_df &lt;- tibble(mins)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  7.86\n 2  8.57\n 3  7.70\n 4  8.19\n 5  7.59\n 6  7.63\n 7  8.05\n 8  9.77\n 9  8.09\n10  7.03\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          12.0         0.723        0.850\n\nlibrary(tidyverse)\n\n## Min for Exponential\n\nn &lt;- 5 # sample size\nlambda &lt;- 0.5\nmu &lt;- 2 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_min &lt;- function(mu, sigma, n) {single_sample &lt;- rexp(n, lambda)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 0.04889386\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n     mins\n    &lt;dbl&gt;\n 1 0.678 \n 2 0.541 \n 3 0.280 \n 4 0.0843\n 5 0.172 \n 6 0.112 \n 7 0.148 \n 8 0.393 \n 9 0.0435\n10 0.0112\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.400         0.161        0.401\n\nlibrary(tidyverse)\n\n## Max for Exponential\n\nn &lt;- 5 # sample size\nlambda &lt;- 0.5\nmu &lt;- 2 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_max &lt;- function(mu, sigma, n) {single_sample &lt;- rexp(n, lambda)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 5.342965\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  2.28\n 2  1.83\n 3  2.94\n 4  3.89\n 5  5.02\n 6  3.98\n 7  5.88\n 8  6.14\n 9  3.63\n10  4.63\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          4.54          5.82         2.41\n\nlibrary(tidyverse)\n\n## Min for Beta\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- (alpha / (alpha+beta))\nsigma &lt;- sqrt((alpha * beta)/((alpha+beta)^2 * (alpha+beta+1)))\ngenerate_samp_min &lt;- function(mu, sigma, n) {single_sample &lt;- rbeta(n, alpha, beta)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 0.5133723\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1 0.575\n 2 0.547\n 3 0.555\n 4 0.667\n 5 0.690\n 6 0.731\n 7 0.625\n 8 0.574\n 9 0.654\n10 0.793\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.648        0.0110        0.105\n\n## Max for Beta\nlibrary(tidyverse)\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- (alpha / (alpha+beta))\nsigma &lt;- sqrt((alpha * beta)/((alpha+beta)^2 * (alpha+beta+1)))\ngenerate_samp_max &lt;- function(mu, sigma, n) {single_sample &lt;- rbeta(n, alpha, beta)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 0.9354073\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmaxs_df &lt;- tibble(mins)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1 0.575\n 2 0.547\n 3 0.555\n 4 0.667\n 5 0.690\n 6 0.731\n 7 0.625\n 8 0.574\n 9 0.654\n10 0.793\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.922       0.00207       0.0455\n\nlibrary(tidyverse)\n\n## CODE FOR POPULATION GRAPHS\n## create population graphs\n\nnorm_df &lt;- tibble(x = seq(3, 17, length.out = 1000),\n                  dens = dnorm(x, mean = 10, sd = 2),\n                  pop = \"normal(10, 4)\")\nunif_df &lt;- tibble(x = seq(7, 13, length.out = 1000),\n                  dens = dunif(x, 7, 13),\n                  pop = \"uniform(7, 13)\")\nexp_df &lt;- tibble(x = seq(0, 10, length.out = 1000),\n                 dens = dexp(x, 0.5),\n                 pop = \"exp(0.5)\")\nbeta_df &lt;- tibble(x = seq(0, 1, length.out = 1000),\n                  dens = dbeta(x, 8, 2),\n                  pop = \"beta(8, 2)\")\n\npop_plot &lt;- bind_rows(norm_df, unif_df, exp_df, beta_df) |&gt;\n  mutate(pop = fct_relevel(pop, c(\"normal(10, 4)\", \"uniform(7, 13)\",\n                                  \"exp(0.5)\", \"beta(8, 2)\")))\n\nggplot(data = pop_plot, aes(x = x, y = dens)) +\n  geom_line() +\n  theme_minimal() +\n  facet_wrap(~ pop, nrow = 1, scales = \"free\") +\n  labs(title = \"Population Distributions for Each Simulation Setting\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n7.67\n7.98\n0.41\n0.65\n\n\n\\(\\text{E}(Y_{max})\\)\n12.31\n11.99\n4.54\n0.92\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n1.34\n0.82\n0.41\n0.11\n\n\n\\(\\text{SE}(Y_{max})\\)\n1.34\n0.84\n2.39\n0.05\n\n\n\n\n\"Question 2\"\n\n[1] \"Question 2\"\n\nlibrary(tidyverse)\nn &lt;- 5  # Sample size\nlambda &lt;- 0.5  # Rate parameter for Exponential distribution\nx &lt;- seq(0, 3, length.out = 1000)\n# Ymin PDF\ndensity_Ymin &lt;- n * lambda * exp(-n * lambda * x)\n# Ymax PDF\ndensity_Ymax &lt;- n * (1 - exp(-lambda * x))^(n-1) * lambda * exp(-lambda * x)\n\nsamp_min_df &lt;- tibble(x, density_Ymin)\nsamp_max_df &lt;- tibble(x, density_Ymax)\n\nggplot(data = samp_min_df, aes(x = x, y = density_Ymin)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(data = samp_max_df, aes(x = x, y = density_Ymax)) +\n  geom_line() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/04-bayes/index.html",
    "href": "posts/04-bayes/index.html",
    "title": "Mini Project 4",
    "section": "",
    "text": "Bayesian Analysis\n\n# Prior 1\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntarget_mean &lt;- 44/66\nalphas &lt;- seq(0.1, 200, length.out = 500)\nbetas &lt;- 0.45 * alphas\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = \n                                 (alphas*betas)/((alphas+betas)^2 * \n                                                   (alphas+betas+1)))\ntarget_var &lt;- 0.05657^2\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\nparam_df\n\n# A tibble: 500 × 4\n   alphas betas   vars dist_to_target\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n 1  0.1   0.045 0.187          0.184 \n 2  0.501 0.225 0.124          0.121 \n 3  0.901 0.406 0.0928         0.0896\n 4  1.30  0.586 0.0741         0.0709\n 5  1.70  0.766 0.0617         0.0585\n 6  2.10  0.946 0.0529         0.0497\n 7  2.50  1.13  0.0462         0.0430\n 8  2.90  1.31  0.0411         0.0379\n 9  3.30  1.49  0.0370         0.0338\n10  3.71  1.67  0.0336         0.0304\n# ℹ 490 more rows\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.4  20.4 0.00320     0.00000468\n\nlibrary(tidyverse)\n## Prior 2\n\nalphas &lt;- seq(0.1, 2000, length.out = 2000)\nbetas &lt;- (alphas * (1 - 0.75)) / 0.75  \n\ntarget_prob &lt;- 0.10\nprob_less_0.7 &lt;- pbeta(0.7, alphas, betas) \n\ntibble(alphas, betas, prob_less_0.7) |&gt;\n  mutate(close_to_target = abs(prob_less_0.7 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_0.7 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1   95.1  31.7        0.0999       0.0000722\n\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\ninformative_alpha_1 &lt;- 45.36\ninformative_beta_1 &lt;- 20.41\ninformative_alpha_2 &lt;- 95.14\ninformative_beta_2 &lt;- 31.71\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_prior_1 &lt;- dbeta(ps, informative_alpha_1,\ninformative_beta_1)\ninformative_prior_2 &lt;- dbeta(ps, informative_alpha_2,\ninformative_beta_2)\nnoninformative_prior &lt;- dbeta(ps,\nnoninformative_alpha, noninformative_beta)\nprior_plot &lt;- tibble(ps, informative_prior_1, informative_prior_2, noninformative_prior) |&gt;\npivot_longer(2:4, names_to = \"distribution\", values_to = \"density\")\nggplot(data = prior_plot, aes(x = ps, y = density, colour = distribution)) +\ngeom_line() +\nscale_colour_viridis_d(end = 0.9) +\ntheme_minimal() +\nlabs(x = \"p\")\n\n\n\n\n\n\n\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\npost_alpha_1 &lt;- 45.36 + 56\npost_beta_1 &lt;- 20.41 + 28\npost_alpha_2 &lt;- 95.14 + 56\npost_beta_2 &lt;- 31.71 + 28\npost_alpha_3 &lt;- 1 + 56\npost_beta_3 &lt;- 1 + 28\n\npost_1 &lt;- dbeta(ps, post_alpha_1,post_beta_1)\npost_2 &lt;- dbeta(ps, post_alpha_2,post_beta_2)\npost_3 &lt;- dbeta(ps, post_alpha_3,post_beta_3)\npost_plot &lt;- tibble(ps, post_1, post_2, post_3) |&gt;\npivot_longer(2:4, names_to = \"distribution\", values_to = \"density\")\nggplot(data = post_plot, aes(x = ps, y = density, colour = distribution)) +\ngeom_line() +\nscale_colour_viridis_d(end = 0.9) +\ntheme_minimal() +\nlabs(x = \"p\")\n\n\n\n\n\n\n\nqbeta(c(0.05,0.95), post_alpha_1,post_beta_1)\n\n[1] 0.612770 0.738079\n\nqbeta(c(0.05,0.95), post_alpha_2,post_beta_2)\n\n[1] 0.6647346 0.7665474\n\nqbeta(c(0.05,0.95), post_alpha_3,post_beta_3)\n\n[1] 0.5772453 0.7440061"
  },
  {
    "objectID": "posts/03-confidence/index.html",
    "href": "posts/03-confidence/index.html",
    "title": "Mini Project 3",
    "section": "",
    "text": "Simulation to Investigate Confidence Intervals"
  },
  {
    "objectID": "posts/02-essay/index.html",
    "href": "posts/02-essay/index.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "A Meaningful Story\nTitle: A Statistician’s Trip To The Mountain\nTerms: Estimator, Parameter, Estimate, Random Variable, Random Sample, Bias, Variance, Consistent, Likelihood\nOne fine evening, an adventurous Laurentian decided to take a trip down to Whiteface Mountain the next morning for a nice powder day. He is very familiar with this drive from his own experiences, but also the experiences of his teammates over the years. He wonders if he has enough time to make it to the mountain for the first lift at 8:30 am and stop at Dunkin Donuts for a breakfast sandwich. His goal is to find out what time he needs to leave. He has taken some stats classes at St. Lawrence, so he feels like he is well-equipped to estimate the likelihood of arriving at the mountain on time while making his pit stop at Dunkin Donuts. A quick Google search provides the Laurentian with a population mean value for the time it takes to make it through a Dunkin Donuts line, which is 2.9 minutes. This parameter, the population mean, could prove to be useful! He knows, however, that with Dunkin Donuts being a very large chain with hundreds of locations across the country, the variance in that time is probably fairly high, especially considering his local Dunkin is in Canton New York. That said, there is some fluctuation with the time spent in that line from day to day, location to location.\nHe figures the good way to estimate the time in the line is by collecting a random sample. Unfortunately, this student does not have access to every Dunkin customer in the USA, so his contacts will have to suffice. To do his best to make the sample random, although it is constrained to his contacts, he asks Siri to pick 10 random contacts from his phone. He then texts each of them to ask how long it takes them to get through a Dunkin drive-through. He gets his responses and takes note of these values. These random variables produce the following values: 5 minutes, 2 minutes, 3.5 minutes, 10 minutes, 1 minute, 2 minutes, 3 minutes, 1 minute, 30 seconds, and 4 minutes. So for his calculations, he will use the sample mean as an estimator for the population mean. He discovers that the estimator produces an estimate of 3.2 minutes. This student is pretty sharp, and he knows that the sample mean tends to be an unbiased estimator of the population mean, that is to say, that the expectation of the sample mean is equal to the population mean. So the slight difference between 3.2 and 2.9 does not worry him in the slightest. There is no reason to believe there is bias in his estimator, so he feels pretty confident about using it to plan his trip. It also confirms that the Canton location seems to be right in line with the population mean. He is getting pretty hungry just thinking about his trip in the morning. He also wants to know if his estimator is consistent, to make sure he maximizes the efficiency of his trip, so he calls up 5 more people to get their input. They reply with the following values: 5,1,2,2.5, and 1.5 minutes. The average here is 2.4, which brings the total average towards the true population mean of 2.9. He is thinking about ordering a sausage egg and cheese on a biscuit on his way to the mountain, which should be good fuel to ski some powder.\nTo cross his T’s and dot his I’s, he wants to look into some of the outliers and plot his data. He notices that the data points form a bell-shaped curve. The astute stats minor recognizes this as a Normal Distribution. He also knows that based on his data, the variance is not too high, in addition, the population mean is derived from a far larger number of data points. This makes sense to him because he is using a sample mean to estimate a population mean. He feels pretty good about his estimate of 2.9 minutes in the lineup, thus making his total travel time 1 hour and 50 minutes, plus 3! So he needs to leave at 6:37 am to make it for the first lift! So he goes ahead and sets his alarm for 6:30 am, packs up his ski bag, and gets ready for an awesome powder day at Whiteface Mountain."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kobe’s Blog",
    "section": "",
    "text": "Mini Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]