[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Reflecting on Mini Projects 1-5\nOver this semester, the mini-projects in this course offered more than just practice; it was a very nice opportunity to test out some of the concepts that we learned in the note packets and labs. With each project, I found that I was challenging myself to actually understand the concepts rather than the classic plug-and-chug method of mathematics that I was used to. Although some projects were harder for me to wrap my head around than others, I found that each of them presented a great method of thinking critically about the things that I was learning, which reinforced my experience in this course.\nEach project tied back directly to a specific section of the course. The first mini-project on sampling distributions gave me a clearer sense of how skew and shape influence statistics like Ymin and Ymax. It was a really solid refresher on some of the concepts that I had seen back during COVID when I took Stat 113. Thinking about skewedness, in comparison to symmetric distributions and how it affects Ymin and Ymax was a clarifying experience. It was very interesting to see how drastically standard error could vary as well, depending on the distribution. In Mini Project 2, I got into my storytelling bag, but it was the fact that I had to refresh my knowledge on concepts that lay the foundation for statistics, which truly made it a fun project. That project helped bring terms from the Estimation section to life and made the whole idea of an estimator feel less abstract by applying it to a story that I created.\nMini Project 3, which focused on confidence intervals and coverage rates, was a turning point, mainly because I struggled with it. The simulation setup was clear, but interpreting the results in a way that felt intuitive was not easy for me. We wanted to see how well our 90% confidence intervals actually hit the true proportion, across various sample sizes and population parameters. But even with that structure, I found myself unsure of how to reason through the result. The Confidence Intervals note packet had shown us how the pivot method works, and touched on how asymptotic behavior stabilizes as n gets large, but applying that to varying widths and interpreting coverage rates across different p-values was just hard to wrap my head around. I could calculate everything, but it didn’t feel like I had a strong intuition. With that being said, I feel that revisiting the concepts in Mini Project 3 now makes a lot more sense; it just challenged me at the time. Struggling through this Mini Project really propelled me to work harder at the ones to come.\nMini Project 4 was where things got more exciting than what we had previously covered, so I have to say that I agree that an entire course could be taught on Bayes. The Bayesian framework felt logical to me, and the way we layered priors with actual match data made the process feel more personalized than any other project. The Bayesian Analysis section helped solidify how to pick prior parameters using either previous data (like Nadal’s past performance) or subjective belief (like the announcer’s claim). It was cool to see how different priors gave us different posteriors and how that played out both numerically and graphically. I especially liked the way the R code gave us the shape of the full posterior distribution, instead of just a point estimate. Being able to visualize the results made it much easier to understand. Pulling on some probability skills allowed for an intuitive interpretation of these images. Despite Bayes being more of a unique section in the course, I felt that it connected the dots for me in terms of understanding the essence of the frequentist framework that we see elsewhere. \nLastly was Mini Project 5, which was probably the most thought-provoking for me. After covering a good portion of the Hypothesis Testing section, this project pushed me to reconsider how we use p-values and what we mean when we call something ‘significant.’ I got to engage with the philosophy of testing, like understanding how you might want to construct a test based on the specific scenario. Thinking about how statistics is not just numbers made me connect to ideas like this, where the numbers mean something. The situational tradeoff between Type I and Type II errors got driven home for me when I did this project. The project forced a good bit of introspection pertaining to my own research, which I felt was very rewarding. Talking about p-values, something that gets introduced in high school level stats, will inevitably be connected to almost all other sections in the course. It was thought-provoking to consider when it is ‘ok’ to use it heavily, and when it isn’t. It also seemed strange to be hopping back into using p-values so heavily after completing this one. \nThere was a pretty clear progression through most of the Mini Projects, especially with Mini Projects 1-3. First, understand variability; then estimate; then build intervals around those estimates. Mini Project 4 and Mini Project 5 felt as if they stood on their own. Mini Project 4 focused on updating beliefs through prior information and posterior probabilities, while Mini Project 5 asked me to interpret the evidence in context. I was convinced pretty well that you should not just draw the line at .05. Now that I’ve completed all five, I’d say my biggest takeaway is that statistics isn’t just about arriving at numbers, it’s about constructing an argument and letting the numbers tell the story. You can calculate a mean or construct a confidence interval, but unless you understand what assumptions you’re making and what those results actually say about the world, it does not deliver. What I gained from these projects was a way of thinking that asks better questions. I started the semester by focusing on more technical concepts, similar to the calculation-heavy teaching style of past Math-Stat courses. Overall, I saw where my understanding was solid, where it needed work, and developed a great toolkit to take with me moving forward."
  },
  {
    "objectID": "posts/05-pvalues/index.html",
    "href": "posts/05-pvalues/index.html",
    "title": "Mini Project 5",
    "section": "",
    "text": "Advantages and Drawbacks of Using p-values\nMathematical Statistics Mini Project 5\nKobe Villeneuve\nQuestion 1\nI believe the author is referencing the point of focusing solely on statistical significance as a measure of validity for research. As mentioned in the opening statements of the paper, two of the ‘Don’ts’ can be summarized as: do not let the p-value sway you in either direction when determining whether something can be derived from the relationships discovered in research. When researchers focus too much on statistically significant relationships, discovery can be lost in that pursuit. I can see it in my research when parsing through volumes of data, searching for ‘significant’ relationships. While these correlations with strong significance bear some value to the research being done, it does not mean I should be discarding the work done that produced higher p-values. I see statistical thinking as using all of the tools in your statistical toolkit, with statistical significance being one of them. Other tools include intuition about the relationships at play, qualitative aspects of the data that may not be accurately explained by the numbers that represent them, and, of course, the complexity of the data. Statistical thinking involves thinking critically about your data and how it is produced. An example of this would be examining trade centrality and global influence. When I examine the relationship between these two sets, there is a clear and significant relationship. However, when I think critically about my ‘statistics’, I can see there is a strong internal impact of economic influence on global influence. So it is clear that, in a sense, these two categories are measuring something similar, so it dampens the significance of the relationship. Conversely, a relationship between trade centrality and GDP per capita was found to have no significance at all. However, this is largely since GDP per capita is an upward sloping trend over time for developed nations, and centrality varies very marginally. The insignificance of the regression analysis does not mean I should discard the analysis entirely, but rather think more about how I can measure the relationship differently. Case in point, I think what the authors are communicating here is that significance is a tool, to be used inharmony with other tools, to allow a statistician (or math honors student) to conduct meaningful research that produces insightful conclusions.\nQuestion 2\nI think that by the first portion of their statement, the author means the p-value communicates what needs to be received. If a reader understands how to interpret a p-value properly (if they took Stat 326, for example), then they would be able to make the proper association between the p-value and the data. To set a line in the sand and say “this side is significant” and “the other side is not” takes away from the richness of the findings. It creates a mental barrier for consumers of the literature or research to discard the insignificant findings, for the simple reason mentioned earlier in the paper of how entangled the statistical and plain English definitions of significance have become. When creating a boundary or a dichotomy, it removes the element of critical thinking when digesting information. It seems like you have already been told that the data is ‘good’ or ‘bad’ because of the connotations associated with words like significant or insignificant. The label creates the dichotomization because it creates the two groups. If you are under, you are safe, if you are over, then you are not. I think the author means that by creating these two groups, there is too much emphasis placed on the categorization and not enough on the rest of the analysis. This is something I encounter when doing my own research, because I have been taught what is ‘good’ and what is ‘bad’, or at least that is how I interpreted the dichotomy throughout my education. It stands to reason that one should deploy the toolkit I mentioned earlier to enhance their comprehension of data.\nQuestion 3\nI agree mostly with this claim presented by the authors. When deciding what to include in published research, findings need to be evaluated on relevance, depth of research, and quality of data. I think that if the data is collected in an unbiased manner, with a large sample size, and all qualitative factors point to the results being relevant, then the findings should be included. You can cook up any number of relationships that have a low p-value, which have nothing to dowith each other, if you examine the qualitative details of the data. Say, for example, we look at coffee consumption and broken bones, which somehow have a strong relationship with a low p-value. If you examine that qualitatively, or think critically about what you are looking at, I am sure you could conclude that you might want to include something more relevant in your publication. However, I do feel that by examining what kind of research you are doing, the error types in the context of the problem present a fair argument for caring about a p-value to a certain degree. If you want to err on the side of caution and bias more towards an error type that is less harmful, like the arsenic concentration example we have looked at, then considering the p-value may help you with that effort. So while I largely believe what the authors are telling us, I think that the p-value is a tool that can be useful at times to make certain decisions.\nQuestion 4\nI agree wholeheartedly with this statement. A one-size-fits-all approach rarely works for anything, statistics aside. When dealing with data, it is often messy, with a great deal of uncertainty at play. I believe that expecting the certainty of one approach that is best is not realistic. The ATOM method they describe makes sense, because it is more a way of thinking that strays away from the binary decision making of a silver bullet solution. I think that it will always be context-specific, since statistical analysis spreads across every single field in the world. You would not be able to follow the same set of rules when dealing with medical research as you would with consumer preferences for a certain type of fabric for shirts. The context will always alter the approach you have to take when making decisions about your results and interpretation of those results, thus, it is not a reasonable expectation to have one singular method or rule to dictate the usefulness of findings.\nQuestion 5\nThe authors reference ‘statistical thoughtfulness’ as a way of creating an approach to statistical analysis that requires deeper thought in all aspects of conducting research. I think this thoughtfulness means to consider why you are analyzing something, how you will do it, how youwill collect the data, and how the results will impact the current thought on your topic. When you consider what has been done before you in addition to where the community stands on a topic, it frees you up to make a calculated approach to your methods. I’ll draw on a personal example to highlight what it means to be statistically thoughtful. When I was thinking about my research question, I needed to first determine what was out there on my topic. There was plenty of writing done on modelling trade networks, as well as more complex mathematical analysis to predict outcomes of certain systems. Once I knew this, I was able to sharpen and define my goal clearly as an exploratory study on the relationships that clique centrality has with other economic indicators. This freed me up to include all relevant economic indicators, because I knew that a study of this manner did not exist in the popular literature, so although only a few variables held statistical significance with each other, I was able to produce meaningful conclusions. My general interpretation of this thoughtfulness is being deliberate in your decision-making to ensure you are contributing to the broader community.\nQuestion 6\nI agree with the authors that there is a problem with the way statistical results are presented. I think that the problem which the authors are referencing is directly related to the interpretation of those who read it. By stating conclusions using significant or confident language, it overstates the certainty of the claim to the reader. If every person on Earth took statistics courses in their educational journey, it might be okay to leave things as they are. The challenge arises when folks with non-statistical backgrounds interpret the research improperly. I think the term ‘compatibility interval’ is also a bit misleading. I believe that adding an interpretation with probabilities, whether the null is true, could be easier for people to interpret, thus making things more digestible to a broader audience. Compatibility does not introduce much more than the other terms in my opinion, but stating things such as “There is a 70% chance that the diet reduces weight” provides the necessary color.\nQuestion 7\n\nFound in Section 2, paragraph 4.\nI find this quote particularly interesting because it feels paradoxical. It made me think about how you would measure the difference between significant and not significant. Maybe a null hypothesis that there is no difference, and an alternative that there is. If you conduct this test and it tells you that the difference is not statistically significant, do you even trust that interpretation? It challenges statistical significance by using statistical significance. I really like how ‘meta’ this quote felt. It also made me think about how you go about proving the validity of analysis. I think that thoughtfulness, openness, and modesty are all great practices in statistical reporting (and life in general). Pre-specified power, novelty, and other measures that you can evaluate also help in determining how you analyze your data - it just seems like quite a lot of legwork to make these adjustments, although I agree they should be made!"
  },
  {
    "objectID": "posts/01-simulation/index.html",
    "href": "posts/01-simulation/index.html",
    "title": "Mini Project 1",
    "section": "",
    "text": "Sampling Distribution of the Sample Minimum and Maximum\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## Min for Normal\n\nn &lt;- 5 # sample size\nmu &lt;- 10 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_min &lt;- function(mu, sigma, n) {single_sample &lt;- rnorm(n, mu, sigma)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 5.441205\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  5.30\n 2  9.18\n 3  7.37\n 4 10.4 \n 5  8.39\n 6  9.45\n 7  9.06\n 8  9.95\n 9  8.02\n10  6.26\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          7.70          1.72         1.31\n\n## Max for Normal\n\nn &lt;- 5 # sample size\nmu &lt;- 10 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_max &lt;- function(mu, sigma, n) {single_sample &lt;- rnorm(n, mu, sigma)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 10.66192\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  11.9\n 2  12.3\n 3  10.3\n 4  12.7\n 5  14.5\n 6  11.8\n 7  10.3\n 8  13.9\n 9  13.6\n10  12.4\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(min_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          12.3          1.73         1.31\n\nlibrary(tidyverse)\n\n## Min for Uniform\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7 \ntheta2 &lt;- 13 \nmu = (theta1+theta2)/2\nsigma = sqrt((theta2-theta1)^2 / 12)\ngenerate_samp_min &lt;- function(mu, sigma, n) {single_sample &lt;- runif(n,theta1, theta2)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 7.669392\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  7.86\n 2  8.57\n 3  7.70\n 4  8.19\n 5  7.59\n 6  7.63\n 7  8.05\n 8  9.77\n 9  8.09\n10  7.03\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          8.00         0.729        0.854\n\n## Max for the Uniform\nlibrary(tidyverse)\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7 \ntheta2 &lt;- 13 \nmu = (theta1+theta2)/2\nsigma = sqrt((theta2-theta1)^2 / 12)\ngenerate_samp_max &lt;- function(mu, sigma, n) {single_sample &lt;- runif(n,theta1, theta2)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 12.65147\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmaxs_df &lt;- tibble(mins)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  7.86\n 2  8.57\n 3  7.70\n 4  8.19\n 5  7.59\n 6  7.63\n 7  8.05\n 8  9.77\n 9  8.09\n10  7.03\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          12.0         0.723        0.850\n\nlibrary(tidyverse)\n\n## Min for Exponential\n\nn &lt;- 5 # sample size\nlambda &lt;- 0.5\nmu &lt;- 2 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_min &lt;- function(mu, sigma, n) {single_sample &lt;- rexp(n, lambda)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 0.04889386\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n     mins\n    &lt;dbl&gt;\n 1 0.678 \n 2 0.541 \n 3 0.280 \n 4 0.0843\n 5 0.172 \n 6 0.112 \n 7 0.148 \n 8 0.393 \n 9 0.0435\n10 0.0112\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.400         0.161        0.401\n\nlibrary(tidyverse)\n\n## Max for Exponential\n\nn &lt;- 5 # sample size\nlambda &lt;- 0.5\nmu &lt;- 2 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_max &lt;- function(mu, sigma, n) {single_sample &lt;- rexp(n, lambda)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 5.342965\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  2.28\n 2  1.83\n 3  2.94\n 4  3.89\n 5  5.02\n 6  3.98\n 7  5.88\n 8  6.14\n 9  3.63\n10  4.63\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          4.54          5.82         2.41\n\nlibrary(tidyverse)\n\n## Min for Beta\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- (alpha / (alpha+beta))\nsigma &lt;- sqrt((alpha * beta)/((alpha+beta)^2 * (alpha+beta+1)))\ngenerate_samp_min &lt;- function(mu, sigma, n) {single_sample &lt;- rbeta(n, alpha, beta)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 0.5133723\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1 0.575\n 2 0.547\n 3 0.555\n 4 0.667\n 5 0.690\n 6 0.731\n 7 0.625\n 8 0.574\n 9 0.654\n10 0.793\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.648        0.0110        0.105\n\n## Max for Beta\nlibrary(tidyverse)\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- (alpha / (alpha+beta))\nsigma &lt;- sqrt((alpha * beta)/((alpha+beta)^2 * (alpha+beta+1)))\ngenerate_samp_max &lt;- function(mu, sigma, n) {single_sample &lt;- rbeta(n, alpha, beta)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 0.9354073\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmaxs_df &lt;- tibble(mins)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1 0.575\n 2 0.547\n 3 0.555\n 4 0.667\n 5 0.690\n 6 0.731\n 7 0.625\n 8 0.574\n 9 0.654\n10 0.793\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.922       0.00207       0.0455\n\nlibrary(tidyverse)\n\n## CODE FOR POPULATION GRAPHS\n## create population graphs\n\nnorm_df &lt;- tibble(x = seq(3, 17, length.out = 1000),\n                  dens = dnorm(x, mean = 10, sd = 2),\n                  pop = \"normal(10, 4)\")\nunif_df &lt;- tibble(x = seq(7, 13, length.out = 1000),\n                  dens = dunif(x, 7, 13),\n                  pop = \"uniform(7, 13)\")\nexp_df &lt;- tibble(x = seq(0, 10, length.out = 1000),\n                 dens = dexp(x, 0.5),\n                 pop = \"exp(0.5)\")\nbeta_df &lt;- tibble(x = seq(0, 1, length.out = 1000),\n                  dens = dbeta(x, 8, 2),\n                  pop = \"beta(8, 2)\")\n\npop_plot &lt;- bind_rows(norm_df, unif_df, exp_df, beta_df) |&gt;\n  mutate(pop = fct_relevel(pop, c(\"normal(10, 4)\", \"uniform(7, 13)\",\n                                  \"exp(0.5)\", \"beta(8, 2)\")))\n\nggplot(data = pop_plot, aes(x = x, y = dens)) +\n  geom_line() +\n  theme_minimal() +\n  facet_wrap(~ pop, nrow = 1, scales = \"free\") +\n  labs(title = \"Population Distributions for Each Simulation Setting\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n7.67\n7.98\n0.41\n0.65\n\n\n\\(\\text{E}(Y_{max})\\)\n12.31\n11.99\n4.54\n0.92\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n1.34\n0.82\n0.41\n0.11\n\n\n\\(\\text{SE}(Y_{max})\\)\n1.34\n0.84\n2.39\n0.05\n\n\n\n\n\"Question 2\"\n\n[1] \"Question 2\"\n\nlibrary(tidyverse)\nn &lt;- 5  # Sample size\nlambda &lt;- 0.5  # Rate parameter for Exponential distribution\nx &lt;- seq(0, 3, length.out = 1000)\n# Ymin PDF\ndensity_Ymin &lt;- n * lambda * exp(-n * lambda * x)\n# Ymax PDF\ndensity_Ymax &lt;- n * (1 - exp(-lambda * x))^(n-1) * lambda * exp(-lambda * x)\n\nsamp_min_df &lt;- tibble(x, density_Ymin)\nsamp_max_df &lt;- tibble(x, density_Ymax)\n\nggplot(data = samp_min_df, aes(x = x, y = density_Ymin)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(data = samp_max_df, aes(x = x, y = density_Ymax)) +\n  geom_line() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/04-bayes/index.html",
    "href": "posts/04-bayes/index.html",
    "title": "Mini Project 4",
    "section": "",
    "text": "Bayesian Analysis\n\n# Prior 1\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntarget_mean &lt;- 44/66\nalphas &lt;- seq(0.1, 200, length.out = 500)\nbetas &lt;- 0.45 * alphas\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = \n                                 (alphas*betas)/((alphas+betas)^2 * \n                                                   (alphas+betas+1)))\ntarget_var &lt;- 0.05657^2\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\nparam_df\n\n# A tibble: 500 × 4\n   alphas betas   vars dist_to_target\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n 1  0.1   0.045 0.187          0.184 \n 2  0.501 0.225 0.124          0.121 \n 3  0.901 0.406 0.0928         0.0896\n 4  1.30  0.586 0.0741         0.0709\n 5  1.70  0.766 0.0617         0.0585\n 6  2.10  0.946 0.0529         0.0497\n 7  2.50  1.13  0.0462         0.0430\n 8  2.90  1.31  0.0411         0.0379\n 9  3.30  1.49  0.0370         0.0338\n10  3.71  1.67  0.0336         0.0304\n# ℹ 490 more rows\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.4  20.4 0.00320     0.00000468\n\nlibrary(tidyverse)\n## Prior 2\n\nalphas &lt;- seq(0.1, 2000, length.out = 2000)\nbetas &lt;- (alphas * (1 - 0.75)) / 0.75  \n\ntarget_prob &lt;- 0.10\nprob_less_0.7 &lt;- pbeta(0.7, alphas, betas) \n\ntibble(alphas, betas, prob_less_0.7) |&gt;\n  mutate(close_to_target = abs(prob_less_0.7 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_0.7 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1   95.1  31.7        0.0999       0.0000722\n\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\ninformative_alpha_1 &lt;- 45.36\ninformative_beta_1 &lt;- 20.41\ninformative_alpha_2 &lt;- 95.14\ninformative_beta_2 &lt;- 31.71\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_prior_1 &lt;- dbeta(ps, informative_alpha_1,\ninformative_beta_1)\ninformative_prior_2 &lt;- dbeta(ps, informative_alpha_2,\ninformative_beta_2)\nnoninformative_prior &lt;- dbeta(ps,\nnoninformative_alpha, noninformative_beta)\nprior_plot &lt;- tibble(ps, informative_prior_1, informative_prior_2, noninformative_prior) |&gt;\npivot_longer(2:4, names_to = \"distribution\", values_to = \"density\")\nggplot(data = prior_plot, aes(x = ps, y = density, colour = distribution)) +\ngeom_line() +\nscale_colour_viridis_d(end = 0.9) +\ntheme_minimal() +\nlabs(x = \"p\")\n\n\n\n\n\n\n\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\npost_alpha_1 &lt;- 45.36 + 56\npost_beta_1 &lt;- 20.41 + 28\npost_alpha_2 &lt;- 95.14 + 56\npost_beta_2 &lt;- 31.71 + 28\npost_alpha_3 &lt;- 1 + 56\npost_beta_3 &lt;- 1 + 28\n\npost_1 &lt;- dbeta(ps, post_alpha_1,post_beta_1)\npost_2 &lt;- dbeta(ps, post_alpha_2,post_beta_2)\npost_3 &lt;- dbeta(ps, post_alpha_3,post_beta_3)\npost_plot &lt;- tibble(ps, post_1, post_2, post_3) |&gt;\npivot_longer(2:4, names_to = \"distribution\", values_to = \"density\")\nggplot(data = post_plot, aes(x = ps, y = density, colour = distribution)) +\ngeom_line() +\nscale_colour_viridis_d(end = 0.9) +\ntheme_minimal() +\nlabs(x = \"p\")\n\n\n\n\n\n\n\nqbeta(c(0.05,0.95), post_alpha_1,post_beta_1)\n\n[1] 0.612770 0.738079\n\nqbeta(c(0.05,0.95), post_alpha_2,post_beta_2)\n\n[1] 0.6647346 0.7665474\n\nqbeta(c(0.05,0.95), post_alpha_3,post_beta_3)\n\n[1] 0.5772453 0.7440061"
  },
  {
    "objectID": "posts/03-confidence/index.html",
    "href": "posts/03-confidence/index.html",
    "title": "Mini Project 3",
    "section": "",
    "text": "Simulation to Investigate Confidence Intervals\n\n## AI Assistance Used\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n###Our Parameters\nsample_sizes &lt;- c(12, 555, 8090)\npopulation_proportions &lt;- c(0.55, 0.15)\nconfidence_level &lt;- 0.90\nz_critical &lt;- qnorm(1 - (1 - confidence_level) / 2)\nn_simulations &lt;- 5000\n\n###Creating the Simulation Function\nrun_simulation &lt;- function(n, p) {\n  coverage_count &lt;- 0\n  interval_widths &lt;- numeric(n_simulations)\n  \n  for (i in seq_len(n_simulations)) {\n    sample &lt;- rbinom(1, n, p) / n  ###Generate sample proportion\n    temp &lt;- replicate(2, sample * (1 - sample))\n    se &lt;- ifelse(mean(temp) &gt; 0, sqrt(mean(temp) / n), 0)  ###Standard error check\n    \n    ###Confidence Interval\n    ci_lower &lt;- sample - z_critical * se\n    ci_upper &lt;- sample + z_critical * se\n    \n    ###Coverage\n    if (p &gt;= ci_lower & p &lt;= ci_upper) {\n      coverage_count &lt;- coverage_count + 1\n    }\n    \n    ###Interval Width\n    interval_widths[i] &lt;- abs(ci_upper - ci_lower)\n  }\n  \n  ###Final Computation\n  coverage_rate &lt;- coverage_count / n_simulations\n  avg_width &lt;- mean(interval_widths)\n  \n  return(tibble(Sample_Size = n, Population_Proportion = p, Coverage_Rate = coverage_rate, Average_Width = avg_width))\n}\n\n###Running Sims\nresults_list &lt;- expand.grid(Sample_Size = sample_sizes, Population_Proportion = population_proportions) %&gt;%\n  pmap(~ run_simulation(..1, ..2))\n\nresults_df &lt;- bind_rows(results_list)\n\nprint(results_df)\n\n# A tibble: 6 × 4\n  Sample_Size Population_Proportion Coverage_Rate Average_Width\n        &lt;dbl&gt;                 &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1          12                  0.55         0.928        0.452 \n2         555                  0.55         0.907        0.0694\n3        8090                  0.55         0.904        0.0182\n4          12                  0.15         0.836        0.295 \n5         555                  0.15         0.900        0.0498\n6        8090                  0.15         0.903        0.0131\n\n\n\n\nWrite Up\nResults Table\n\n\n\n\n\nSample Size\nPopulation Proportion\nCoverage Rate\nAverage Width\n\n\n12\n0.55\n0.9256\n0.45099\n\n\n555\n0.55\n0.9062\n0.06940\n\n\n8090\n0.55\n0.8928\n0.01819\n\n\n12\n0.15\n0.8290\n0.29356\n\n\n555\n0.15\n0.8982\n0.04976\n\n\n8090\n0.15\n0.9094\n0.01305\n\n\n\n\n\nLarge Sample Assumption Calculation\n\n\n\nIf we take a look at the selected sample sizes, I went for a very small sample of 12, a large sample of 555, and a very large sample of 8090. If we examine both settings where the sample is 12, we can see a considerable deviation from the ideal coverage rate we are looking for. Since the confidence interval we are working with is 90%, we want that coverage rate to approach that value as well. However, as we can see with the assumption calculation, it violates the large n assumption at least once for each of those settings. This lets us know that the value of n is too small. It is also worth noting that both the p = 0.15 and p = 0.55 have considerable average widths, indicating a lack of precision when it comes to the Confidence Interval. Interestingly, the average width is larger for p = 0.55 compared to p = 0.15. \nMoving on to the next sample size of n = 555, we can see a few stark differences between those two settings and the small n setting. First of all, we can see that the coverage rates for both of these settings (p = 0.15 and p = 0.55) are extremely close to the desired value of 0.90. Additionally, when we examine the results of the large n test, we can see that each of these two settings passes. The difference from 0.9 in the coverage rate for each setting is negligible, with one slightly higher and the other slightly lower. More interesting to note is that we can see a continued trend of the average width being wider for the setting that is closer to p = 0.5. Although both settings saw a dramatic ‘improvement’ in their coverage rates, it is still true that the setting of p = 0.55 has a wider average width. \nLastly, when examining the final two settings where n = 8090, a few additional trends emerge, while some are also confirmed. The coverage rates are about the same as the previous setting when looking at how far off they are from 0.9. However, if we look closely, we can see that the coverage rate for the setting of p = 0.55 is descending as n increases, whereas the setting of p = 0.15 is ascending as n increases. We can also confirm what was observed in the previous 4 settings, where no matter the n, we see a larger average width in the setting closer to p = 0.5. However, as the value of n increases, that discrepancy becomes less and less apparent."
  },
  {
    "objectID": "posts/02-essay/index.html",
    "href": "posts/02-essay/index.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "A Meaningful Story\nTitle: A Statistician’s Trip To The Mountain\nTerms: Estimator, Parameter, Estimate, Random Variable, Random Sample, Bias, Variance, Consistent, Likelihood\nOne fine evening, an adventurous Laurentian decided to take a trip down to Whiteface Mountain the next morning for a nice powder day. He is very familiar with this drive from his own experiences, but also the experiences of his teammates over the years. He wonders if he has enough time to make it to the mountain for the first lift at 8:30 am and stop at Dunkin Donuts for a breakfast sandwich. His goal is to find out what time he needs to leave. He has taken some stats classes at St. Lawrence, so he feels like he is well-equipped to estimate the likelihood of arriving at the mountain on time while making his pit stop at Dunkin Donuts. A quick Google search provides the Laurentian with a population mean value for the time it takes to make it through a Dunkin Donuts line, which is 2.9 minutes. This parameter, the population mean, could prove to be useful! He knows, however, that with Dunkin Donuts being a very large chain with hundreds of locations across the country, the variance in that time is probably fairly high, especially considering his local Dunkin is in Canton New York. That said, there is some fluctuation with the time spent in that line from day to day, location to location.\nHe figures the good way to estimate the time in the line is by collecting a random sample. Unfortunately, this student does not have access to every Dunkin customer in the USA, so his contacts will have to suffice. To do his best to make the sample random, although it is constrained to his contacts, he asks Siri to pick 10 random contacts from his phone. He then texts each of them to ask how long it takes them to get through a Dunkin drive-through. He gets his responses and takes note of these values. These random variables produce the following values: 5 minutes, 2 minutes, 3.5 minutes, 10 minutes, 1 minute, 2 minutes, 3 minutes, 1 minute, 30 seconds, and 4 minutes. So for his calculations, he will use the sample mean as an estimator for the population mean. He discovers that the estimator produces an estimate of 3.2 minutes. This student is pretty sharp, and he knows that the sample mean tends to be an unbiased estimator of the population mean, that is to say, that the expectation of the sample mean is equal to the population mean. So the slight difference between 3.2 and 2.9 does not worry him in the slightest. There is no reason to believe there is bias in his estimator, so he feels pretty confident about using it to plan his trip. It also confirms that the Canton location seems to be right in line with the population mean. He is getting pretty hungry just thinking about his trip in the morning. He also wants to know if his estimator is consistent, to make sure he maximizes the efficiency of his trip, so he calls up 5 more people to get their input. They reply with the following values: 5,1,2,2.5, and 1.5 minutes. The average here is 2.4, which brings the total average towards the true population mean of 2.9. He is thinking about ordering a sausage egg and cheese on a biscuit on his way to the mountain, which should be good fuel to ski some powder.\nTo cross his T’s and dot his I’s, he wants to look into some of the outliers and plot his data. He notices that the data points form a bell-shaped curve. The astute stats minor recognizes this as a Normal Distribution. He also knows that based on his data, the variance is not too high, in addition, the population mean is derived from a far larger number of data points. This makes sense to him because he is using a sample mean to estimate a population mean. He feels pretty good about his estimate of 2.9 minutes in the lineup, thus making his total travel time 1 hour and 50 minutes, plus 3! So he needs to leave at 6:37 am to make it for the first lift! So he goes ahead and sets his alarm for 6:30 am, packs up his ski bag, and gets ready for an awesome powder day at Whiteface Mountain."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kobe’s Blog",
    "section": "",
    "text": "Mini Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]